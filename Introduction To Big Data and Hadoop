

## **Types of Digital Data**

Digital data is the information that is stored, processed, and transferred using computers. It is mainly divided into **three types**:

### **1. Structured Data**

* This is **well-organized data** that follows a fixed format.
* It is stored in rows and columns like in **databases, spreadsheets**.
* Easy to search, store and analyze.
* **Examples:** Student records, bank transactions, employee details.

### **2. Unstructured Data**

* This data **does not have a fixed format**.
* It is large, messy and difficult to analyze directly.
* It includes text, images, audio, video, social media posts, emails, etc.
* **Examples:** YouTube videos, WhatsApp messages, tweets.

### **3. Semi-structured Data**

* This data is **partly organized** but not in strict rows and columns.
* It has tags or markers that make it easier to understand.
* Often used in web applications.
* **Examples:** XML files, JSON data, HTML pages.

---

### **Short Conclusion**

Structured, unstructured, and semi-structured data are the three main types of digital data. They differ in format, organization, and how easy they are to process.

---

If you want, I can also give **short 3-mark** or **long 6-mark** versions.







2. Introduction to Big Data (3–6 marks)

Big Data refers to extremely large and complex datasets that cannot be processed using traditional data processing tools.

Key Characteristics (5 Vs):

Volume – Huge amount of data generated every second.

Velocity – Speed at which data is generated and processed.

Variety – Different types of data (structured, semi-structured, unstructured).

Veracity – Quality and accuracy of data.

Value – Useful insights gained from data.

Why Big Data is Important

Helps organizations make better decisions.

Used to identify patterns and trends.

Supports real-time analytics.

Examples

Social media data (likes, posts, comments)




Common Unix tools used:

grep – search text in files

awk – pattern scanning and processing

sed – text editing

sort – sorting data

uniq – removing duplicates

wc – word or line count

Why Unix tools were used:

They are fast and efficient for text processing.

Useful for small and medium-sized data sets.

Easy to automate using shell scripts.

Sensor data from IoT devices




Here are **easy, exam-ready answers** for **Hadoop Streaming** and **Hadoop Ecosystem**, suitable for **3–6 marks**.

---

# **1. Hadoop Streaming (3–6 marks)**

**Hadoop Streaming** is a tool provided with Hadoop that allows you to write **MapReduce programs using any programming language**, not only Java.

### **Key Points**

* Normally, MapReduce is written in **Java**, but Hadoop Streaming lets you use **Python, C++, Ruby, Shell Script**, etc.
* It works by sending input data to your script via **standard input (stdin)** and reading the output from **standard output (stdout)**.
* Your script acts like a **mapper** or **reducer**.

### **How it Works**

1. Write a simple script (example: Python) that reads text and prints key-value pairs.
2. Run the script using the Hadoop Streaming command.
3. Hadoop takes your script as mapper/reducer and processes data on the cluster.

### **Advantages**

* Easy to write simple data analysis programs.
* No need to learn Java for MapReduce.
* Good for rapid development and testing.
* Supports all languages that can read and write text.

### **Example**

Word count in Python can be used as a MapReduce job through Hadoop Streaming.

### **Short Answer**

Hadoop Streaming is a utility that lets you run MapReduce jobs using any language that can read/write text. It uses stdin/stdout to communicate between Hadoop and the user script.

---

# **2. Hadoop Ecosystem (3–6 marks)**

The **Hadoop Ecosystem** refers to all the tools, frameworks, and technologies built around Hadoop to store, process, analyse, and manage big data.

### **Main Components of the Hadoop Ecosystem**

#### **1. Storage Layer**

* **HDFS (Hadoop Distributed File System)** – Stores huge data across multiple machines.
* **HBase** – NoSQL database for real-time read/write.

#### **2. Processing Layer**

* **MapReduce** – Batch data processing.
* **Apache Spark** – Fast in-memory processing.
* **Tez** – Faster processing engine for Hive and Pig.

#### **3. Data Analysis Tools**

* **Hive** – SQL-like queries for big data.
* **Pig** – Scripting language for big data analysis.

#### **4. Data Ingestion Tools**

* **Sqoop** – Transfer data between Hadoop and relational databases.
* **Flume** – Collects streaming data (logs, social media).

#### **5. Resource Management**

* **YARN (Yet Another Resource Negotiator)** – Allocates resources and manages cluster jobs.

#### **6. Workflow Scheduling**

* **Oozie** – Schedules and manages Hadoop jobs.
* **Zookeeper** – Coordinates distributed applications.

#### **7. Search and Indexing**

* **Solr / Lucene** – Search engine for big data.

### **Short Answer**

The Hadoop Ecosystem is a collection of tools such as HDFS, MapReduce, YARN, Hive, Pig, HBase, Sqoop, Flume, and Spark that work together to store, process, manage, and analyse large datasets.

---

If you want, I can also give you **IBM Big Data Strategy**, **Infosphere BigInsights**, and **BigSheets** answers in exam format.


Banking and e-commerce transactions
